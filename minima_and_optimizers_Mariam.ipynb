{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "minima_and_optimizers_Mariam.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "V8q1athm5kYk"
      ]
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMMUKafX5kXg",
        "colab_type": "text"
      },
      "source": [
        "## Which minima are preferred by which optimizers?\n",
        "#### Project for OptML course at EPFL\n",
        "\n",
        "When training a neural net, the resulting stationary point $\\nabla f(x)=0$ depends on many things like initialization, optimization algorithm, batch size, learning rate and others. It was hypothesized in 1997 [1] that not all minima of a neural network are the same when it comes to generalization capabilities. In particular it was conjectured that the more a minimum is *flat*, meaning that Hessian $\\frac{\\partial f}{\\partial \\theta^2}$ eigenvalues are small, the better network generalizes. The reasoning is that a more flat minimum means that parameter perturbation would not change (spoil) the loss a lot.\n",
        "\n",
        "The problem with this approach was pointed out in [2]. It was shown that a fixed network can be reparametrized such that it has arbitrary sharpness. Therefore the function represented is the same (and having same generalization capabilities) but sharpness would be different.\n",
        "\n",
        "Current research focuses on trying to quantify sharpness in another form. For example, paper [4] visualizes the loss landscape in two random directions and proposes to normalize the weights. In [3] another idea is considered: cosine similarity between gradients on different batches is considered as a new generalization metric.\n",
        "\n",
        "The paper [5] uses another normalized sharpness metric and shows that it correlates with generalization gap well. PAC-Bayesian approach tries to quantify the difference between test and train errors using KL divergence between these distributions. The paper uses a mix between Hessian diagonal values and weight matrix norms.\n",
        "\n",
        "Paper [6] tackles the problem by estimating Hessian spectral density empirically using techniques allowing for computational feasibility for large networks.\n",
        "\n",
        "Paper [7] proposes another scale-invariant generalization metric.\n",
        "\n",
        "We note that the papers usually study the effect of different batch sizes on the generalization properties. In this project we compare the effect of **optimizer** on the sharpness metrics as well as on generalization properties.\n",
        "\n",
        "We propose to use the following datsets for our experiment:\n",
        "1. Fisher's Iris dataset (150 objects, 5 features, 3 classes)\n",
        "2. Boston Housing dataset (~500 objects, 15 features, regression)\n",
        "3. MNIST dataset (70000 objects, 28x28 features, 10 classes)\n",
        "\n",
        "We will use the following [optimizers](https://keras.io/optimizers/):\n",
        "1. (S)GD\n",
        "2. SGD with momentum\n",
        "3. AdaDelta\n",
        "\n",
        "And will vary the following parameters:\n",
        "1. Batch size\n",
        "2. Learning rate\n",
        "3. [Initialization](https://keras.io/initializers/) (Normal, Uniform, Orthogonal, LeCun, Glorot)\n",
        "\n",
        "We plan to measure the following metrics:\n",
        "1. Empirical generalization gap\n",
        "2. Hessian spectrum\n",
        "3. Proposed metric from [3]\n",
        "4. Proposed metric from [4]\n",
        "5. Proposed metric from [5]\n",
        "6. Proposed metric from [7]\n",
        "\n",
        "Our main contribution would be to **measure the effect of optimizer on minima quality**\n",
        "\n",
        "[1] Hochreiter, Sepp and Schmidhuber, Jürgen. [Flat minima](http://www.bioinf.jku.at/publications/older/3304.pdf). Neural Computation, 9(1):1–42, 1997.\n",
        "\n",
        "[2] Dinh, Laurent, et al. [Sharp minima can generalize for deep nets](https://arxiv.org/pdf/1703.04933.pdf). Proceedings of ICML-Volume 70. JMLR. org, 2017.\n",
        "\n",
        "[3] [The Generalization Mystery: Sharp vs Flat Minima](https://www.inference.vc/sharp-vs-flat-minima-are-still-a-mystery-to-me/)\n",
        "\n",
        "[4] Li H, Xu Z, Taylor G, Studer C, Goldstein T. [Visualizing the loss landscape of neural nets](https://papers.nips.cc/paper/7875-visualizing-the-loss-landscape-of-neural-nets.pdf). In NeurIPS 2018 (pp. 6391-6401).\n",
        "\n",
        "[5] Tsuzuku Y, Sato I, Sugiyama M. [Normalized Flat Minima: Exploring Scale Invariant Definition of Flat Minima for Neural Networks using PAC-Bayesian Analysis](https://arxiv.org/pdf/1901.04653.pdf). arXiv preprint arXiv:1901.04653. 2019 Jan 15.\n",
        "\n",
        "[6] Ghorbani B, Krishnan S, Xiao Y. [An Investigation into Neural Net Optimization via Hessian Eigenvalue Density](https://arxiv.org/pdf/1901.10159.pdf). arXiv preprint arXiv:1901.10159. 2019 Jan 29.\n",
        "\n",
        "[7] Rangamani A, Nguyen NH, Kumar A, Phan D, Chin SH, Tran TD. [A Scale Invariant Flatness Measure for Deep Network Minima](https://arxiv.org/pdf/1902.02434.pdf). arXiv preprint arXiv:1902.02434. 2019 Feb 6."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfWjrB0X5kXh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdHJ4Qzv5kXk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for environ\n",
        "import os\n",
        "\n",
        "# only using device 0\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   # see issue #152\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
        "\n",
        "# importing tensorflow\n",
        "import tensorflow as tf\n",
        "\n",
        "# import scipy\n",
        "import scipy.misc, csv"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "kutNQ5yQ5kXm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "26df3520-4fa9-465c-fc02-a99de2cc75f2"
      },
      "source": [
        "!rm iris.csv\n",
        "!wget https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-05-30 14:36:26--  https://raw.githubusercontent.com/mwaskom/seaborn-data/master/iris.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3858 (3.8K) [text/plain]\n",
            "Saving to: ‘iris.csv’\n",
            "\n",
            "\riris.csv              0%[                    ]       0  --.-KB/s               \riris.csv            100%[===================>]   3.77K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-05-30 14:36:26 (55.2 MB/s) - ‘iris.csv’ saved [3858/3858]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZKMF8Ib5kXp",
        "colab_type": "text"
      },
      "source": [
        "Using feedforward fully-connected neural network with input size 28^2, 5 layers (28^2-40-40-30-30-10-1), 10-dim output (classification). Using MNIST as data. Using optimizers: SGD, Adam. Plotting spectra of the hessian after convergence. Using 10 repetitions of the experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "4VR0An_35kXq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loading mnist\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "qEe43dvf5kXs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "sXYyi-2X5kXu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        },
        "outputId": "4c920da1-93dc-45dd-b259-0ab56ee6f4fe"
      },
      "source": [
        "plt.imshow(x_train[9])"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f7eb142ce10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nlkRN4JIGcrAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSRm6yFu5kXx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.reset_default_graph()\n",
        "# allowing GPU memory growth to allocate only what we need\n",
        "config = tf.ConfigProto(log_device_placement=True)\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.5\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.Session(config=config, graph = tf.get_default_graph())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI6fgPwX5kXy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "iris = list(csv.reader(open('iris.csv')))[1:]\n",
        "\n",
        "# The inputs are four floats: sepal length, sepal width, petal length, petal width.\n",
        "inputs  = np.array(iris)[:,:4].astype(np.float)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7KUtjly5kX1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "outputs = np.array(iris)[:,4]\n",
        "# Convert the output strings to ints.\n",
        "outputs_vals, outputs_ints = np.unique(outputs, return_inverse=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DQAiRF7f5kX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split the input and output data sets into training and test subsets.\n",
        "inds = np.random.permutation(len(inputs))\n",
        "train_inds, test_inds = np.array_split(inds, 2)\n",
        "x_train, y_train = inputs[train_inds], outputs_ints[train_inds]\n",
        "x_test,  y_test  = inputs[test_inds],  outputs_ints[test_inds]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSL0RYT15kX5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# input data: batch w h channels\n",
        "x = tf.placeholder(tf.float32, shape = (None, 4), name = 'input')\n",
        "\n",
        "# output labels (vector)\n",
        "y = tf.placeholder(tf.int64, shape = (None,), name = 'labels')\n",
        "\n",
        "# one-hot encoded labels\n",
        "y_one_hot = tf.one_hot(y, 3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IG2V6N8v5kX7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fc_layer(x, n, activation = tf.nn.sigmoid):\n",
        "    \"\"\" Fully connected layer for input x and output dim n \"\"\"\n",
        "    return tf.contrib.layers.fully_connected(x, n, activation_fn=activation,\n",
        "    weights_initializer=tf.initializers.lecun_normal(), weights_regularizer=None,\n",
        "    biases_initializer=tf.zeros_initializer(), biases_regularizer=None, trainable=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoycOziw5kX9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# flatten w * h\n",
        "l0 = tf.contrib.layers.flatten(x)\n",
        "\n",
        "# layers\n",
        "with tf.name_scope('layers'):\n",
        "    z = l0\n",
        "    z = fc_layer(z, 10)\n",
        "    z = fc_layer(z, 5)\n",
        "    z = fc_layer(z, 3, activation = None)\n",
        "    output = z\n",
        "\n",
        "# softmax to make probability distribution\n",
        "logits = tf.nn.softmax(output)\n",
        "\n",
        "# predicted labels\n",
        "labels = tf.argmax(logits, axis = 1)\n",
        "\n",
        "# loss: cross-entropy\n",
        "loss = tf.losses.softmax_cross_entropy(y_one_hot, logits)\n",
        "\n",
        "# accuracy of predictions\n",
        "accuracy = tf.contrib.metrics.accuracy(labels, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tmERaxbk5kX_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# list of all parameters\n",
        "params = tf.trainable_variables()#tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
        "dt = tf.trainable_variables()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XIxStjOa5kYA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "82ca7ee4-c568-4c25-9fb5-968064e8e0bf"
      },
      "source": [
        "print('Total parameters:', np.sum([np.prod(p.shape) for p in params]))"
      ],
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Total parameters: 123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jk797XQL5kYC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class OwnGradientDescent():\n",
        "    def __init__(self, gamma = 0.5, theta = 0.9):\n",
        "        # gamma (learning rate)\n",
        "        self.gamma = tf.Variable(gamma, dtype = tf.float32)\n",
        "        self.theta = theta\n",
        "        \n",
        "    def minimize(self, loss, params):\n",
        "        \"\"\" Minimize some loss \"\"\"\n",
        "        def decrement_weights(W, gamma, grads):\n",
        "            \"\"\" w = w - how_much \"\"\"\n",
        "            ops = [w.assign(tf.subtract(w, tf.multiply(gamma, grad))) for w, grad in zip(W, grads)]\n",
        "            return tf.group(ops)\n",
        "        \n",
        "        # gradients of the loss w.r.t. params\n",
        "        grads = tf.gradients(loss, params)\n",
        "        \n",
        "        # perform gradient descent step\n",
        "        train_op = decrement_weights(params, self.gamma, grads)\n",
        "        \n",
        "        # updating gamma\n",
        "        upd_op = self.gamma.assign(tf.multiply(self.gamma, self.theta))\n",
        "        \n",
        "        return tf.group(train_op, upd_op)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cdn7iPI6mQE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class FrankWolfe():\n",
        "  def __init__(self, R = 0.5, p = 3.1, gamma = 0.5):\n",
        "        # gamma (learning rate)\n",
        "        \n",
        "        self.R = R\n",
        "        self.p = p\n",
        "       \n",
        "        q =  1. / (1 - 1. / self.p)\n",
        "            \n",
        "        self.q = q\n",
        "        self.gamma = gamma\n",
        "        print(self.p, self.q, self.R, self.gamma)\n",
        "  def minimize(self, loss, weights):\n",
        "        \"\"\" Minimize some loss \"\"\"\n",
        "        def decrement_weights(weights, gamma, descent_direction):\n",
        "            \"\"\" w = w - how_much \"\"\"\n",
        "            # implementing FW update x = (1 - gamma_t) * x + gamma_t * s_t\n",
        "            frank_wolfe_op = tf.group([w.assign((1. - self.gamma) * w + self.gamma * s)\n",
        "                           for w, s in zip(weights, descent_direction)])\n",
        "            return tf.group(frank_wolfe_op)\n",
        "        def get_p_vector_norm(list_of_tensors, order):\n",
        "            \"\"\" p-norm of a list of tensors flattened to a vector \"\"\"\n",
        "            # weights as a vector\n",
        "            weights_flattened = tf.concat([tf.reshape(w, (-1,)) for w in list_of_tensors], axis = 0)\n",
        "\n",
        "            # p-norm of the weights (must be <= R)\n",
        "            weight_p_norm = tf.norm(weights_flattened, ord = order)\n",
        "            return weight_p_norm\n",
        "          \n",
        "        def LMO(g):\n",
        "            \"\"\" Linear oracle for a list of tensors \"\"\"\n",
        "\n",
        "            # overall norm\n",
        "            g_q_qp_norm = get_p_vector_norm(g, self.q) ** (self.q / self.p)\n",
        "\n",
        "            g_qp = [tf.abs(g0) ** (self.q / self.p) for g0 in g]\n",
        "            g_qp_signed = [tf.multiply(g_qp0, tf.sign(g0)) for g_qp0, g0 in zip(g_qp, g)]\n",
        "\n",
        "            return [-self.R * g_qp_signed0 / g_q_qp_norm for g_qp_signed0 in g_qp_signed]\n",
        "        \n",
        "        # loss gradient\n",
        "        grads = tf.gradients(loss, weights)\n",
        "        \n",
        "        # p-norm of weights\n",
        "        weight_p_norm = get_p_vector_norm(weights, self.p)\n",
        "\n",
        "        # gradients p-norm\n",
        "        grad_p_norm = get_p_vector_norm(grads, self.p)\n",
        "\n",
        "         # slack of constraint\n",
        "        constraint_slack = self.R - weight_p_norm\n",
        "        \n",
        "        descent_direction = LMO(grads)\n",
        "        \n",
        "        \n",
        "        op = decrement_weights(weights, self.gamma, descent_direction)\n",
        "        #op = decrement_weights(weights, self.gamma, grads)\n",
        "        \n",
        "        return op"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5itvjTBM5kYI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8f8aae70-10c0-4405-f10d-735e98941eb4"
      },
      "source": [
        "fw  = FrankWolfe(p = 2, R = 100, gamma = 0.01)\n",
        "train_op = fw.minimize(loss, params)\n",
        "def epoch(train_op):\n",
        "    # do the training\n",
        "    train_loss, train_acc, _ = sess.run([loss, accuracy, train_op], feed_dict = {x: x_train, y: y_train})\n",
        "\n",
        "    # compute accuracy and loss\n",
        "    test_loss, test_acc = sess.run([loss, accuracy], feed_dict = {x: x_test, y: y_test})\n",
        "\n",
        "    print(train_loss, train_acc)"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 2.0 100 0.01\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HvrUQarWO7f_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class StochasticFrankWolfe():\n",
        "  def __init__(self, R = 0.5, p = 3.1, gamma = 0.5, ro = 0.5):\n",
        "        # gamma (learning rate)\n",
        "        self.R = R\n",
        "        self.p = p\n",
        "        q =  1. / (1 - 1. / self.p)\n",
        "        self.q = q\n",
        "        self.gamma = gamma\n",
        "        self.ro = ro\n",
        "  def minimize(self, loss, weights):\n",
        "    \n",
        "        dt = [tf.Variable(x) for x in [tf.zeros_like(g) for g in params]]\n",
        "\n",
        "    \n",
        "        \"\"\" Minimize some loss \"\"\"\n",
        "        def decrement_weights(weights, gamma, descent_direction):\n",
        "            \"\"\" w = w - how_much \"\"\"\n",
        "            # implementing FW update x = (1 - gamma_t) * x + gamma_t * s_t\n",
        "            frank_wolfe_op = tf.group([w.assign((1. - self.gamma) * w + self.gamma * s)\n",
        "                           for w, s in zip(weights, descent_direction)])\n",
        "            return tf.group(frank_wolfe_op)\n",
        "        def get_p_vector_norm(list_of_tensors, order):\n",
        "            \"\"\" p-norm of a list of tensors flattened to a vector \"\"\"\n",
        "            # weights as a vector\n",
        "            weights_flattened = tf.concat([tf.reshape(w, (-1,)) for w in list_of_tensors], axis = 0)\n",
        "\n",
        "            # p-norm of the weights (must be <= R)\n",
        "            weight_p_norm = tf.norm(weights_flattened, ord = order)\n",
        "            return weight_p_norm\n",
        "          \n",
        "        def LMO(g):\n",
        "            \"\"\" Linear oracle for a list of tensors \"\"\"\n",
        "\n",
        "            # overall norm\n",
        "            g_q_qp_norm = get_p_vector_norm(g, self.q) ** (self.q / self.p)\n",
        "\n",
        "            g_qp = [tf.abs(g0) ** (self.q / self.p) for g0 in g]\n",
        "            g_qp_signed = [tf.multiply(g_qp0, tf.sign(g0)) for g_qp0, g0 in zip(g_qp, g)]\n",
        "\n",
        "            return [-self.R * g_qp_signed0 / g_q_qp_norm for g_qp_signed0 in g_qp_signed]\n",
        "        \n",
        "        # loss gradient\n",
        "        grads = tf.gradients(loss, weights)\n",
        "        # p-norm of weights\n",
        "        weight_p_norm = get_p_vector_norm(weights, self.p)\n",
        "\n",
        "        # gradients p-norm\n",
        "        grad_p_norm = get_p_vector_norm(grads, self.p)\n",
        "\n",
        "         # slack of constraint\n",
        "        constraint_slack = self.R - weight_p_norm\n",
        "        \n",
        "        dt_next = [(1 - self.ro) * dt1 + self.ro * grad1 for dt1, grad1 in zip(dt, grads)]\n",
        "        dt_op = tf.group([dt1.assign(dtnext1) for dt1, dtnext1 in zip(dt, dt_next)])\n",
        "\n",
        "        descent_direction = LMO(dt_next)\n",
        "        \n",
        "        op = decrement_weights(weights, self.gamma, descent_direction)\n",
        "        #op = decrement_weights(weights, self.gamma, grads)\n",
        "        \n",
        "        return tf.group([dt_op, op])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lw_vh3k4Pwr6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "4c7202ec-6b70-48d8-879f-1e3bdb82bbed"
      },
      "source": [
        "sfw  = StochasticFrankWolfe(p = 2, R = 100, gamma = 0.01, ro = 0.6)\n",
        "sfw_op = sfw.minimize(loss, params)\n",
        "def epoch(train_op):\n",
        "    # do the training\n",
        "    train_loss, train_acc, _ = sess.run([loss, accuracy, sfw_op], feed_dict = {x: x_train, y: y_train})\n",
        "\n",
        "    # compute accuracy and loss\n",
        "    test_loss, test_acc = sess.run([loss, accuracy], feed_dict = {x: x_test, y: y_test})\n",
        "\n",
        "    print(train_loss, train_acc)\n",
        "    \n",
        "\n",
        "metrics = {}\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for _ in range(10):\n",
        "  epoch(sfw_op)"
      ],
      "execution_count": 165,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1274841 0.30666667\n",
            "1.098171 0.33333334\n",
            "1.1010288 0.36\n",
            "1.0574143 0.64\n",
            "1.0132359 0.64\n",
            "1.0366044 0.37333333\n",
            "0.9788781 0.6\n",
            "0.9862625 0.64\n",
            "0.91437644 0.8\n",
            "0.8919395 0.64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiEaJFwJNSFi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "c2ac472f-901c-460f-e68b-3a485800b3c7"
      },
      "source": [
        "metrics = {}\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for _ in range(10):\n",
        "  epoch(train_op)"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1081758 0.32\n",
            "1.1081136 0.36\n",
            "1.1126733 0.32\n",
            "1.0899311 0.36\n",
            "1.0708361 0.50666666\n",
            "1.057639 0.36\n",
            "1.0454614 0.68\n",
            "1.0434017 0.36\n",
            "1.0370253 0.6533333\n",
            "1.0169593 0.36\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4W-CGD0pM3ws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gd  = OwnGradientDescent(gamma = 0.01)\n",
        "gd_op = gd.minimize(loss, params)\n",
        "def epoch(train_op):\n",
        "    # do the training\n",
        "    train_loss, train_acc, _ = sess.run([loss, accuracy, gd_op], feed_dict = {x: x_train, y: y_train})\n",
        "\n",
        "    # compute accuracy and loss\n",
        "    test_loss, test_acc = sess.run([loss, accuracy], feed_dict = {x: x_test, y: y_test})\n",
        "\n",
        "    print(train_loss, train_acc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c1zFfdlHImV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "b371b9a5-b45d-4c47-f054-0baccb5e7830"
      },
      "source": [
        "metrics = {}\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for _ in range(10):\n",
        "  epoch(gd_op)"
      ],
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.1294502 0.32\n",
            "1.1293755 0.32\n",
            "1.1293086 0.32\n",
            "1.1292483 0.32\n",
            "1.1291939 0.32\n",
            "1.1291453 0.32\n",
            "1.1291015 0.32\n",
            "1.1290619 0.32\n",
            "1.1290265 0.32\n",
            "1.1289947 0.32\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6iNZF_fLNd4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0n07m7dKBkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebOFnc0T5kYJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def dz_dw_flatten(z):\n",
        "    \"\"\" Calculate dz/dparams and flatten the result \"\"\"\n",
        "    return tf.concat([tf.reshape(x, shape = (-1,)) for x in tf.gradients(z, params)], axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fi_xuCOL5kYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def iterate_flatten(tensor):\n",
        "    \"\"\" Iterate over flattened items of a tensor \"\"\"\n",
        "    if type(tensor) == list:\n",
        "        for t in tensor:\n",
        "            for v in iterate_flatten(t):\n",
        "                yield v\n",
        "    elif len(tensor.shape) == 0:\n",
        "        yield tensor\n",
        "    else:\n",
        "        for idx in range(tensor.shape[0]):\n",
        "            for v in iterate_flatten(tensor[idx]):\n",
        "                yield v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIIE1jZP5kYO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gradients of the loss w.r.t. params\n",
        "grads = tf.gradients(loss, params)\n",
        "grad_components = list(iterate_flatten(grads))\n",
        "hessian = [dz_dw_flatten(t) for t in tqdm(grad_components)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSel4RpC5kYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metrics = {}\n",
        "def train_and_get_hessian(operation, epochs_):\n",
        "    # metrics to compute\n",
        "    global metrics\n",
        "    metrics = {'train_loss': [], 'test_loss': [], 'train_acc': [], 'test_acc': []}\n",
        "    \n",
        "    # initializing weights...\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "    \n",
        "    for i in tqdm(range(epochs_)):\n",
        "        epoch(operation)\n",
        "    \n",
        "    clear_output()\n",
        "    \n",
        "    plt.figure()\n",
        "    plt.plot(metrics['train_loss'], label = 'train_loss')\n",
        "    plt.plot(metrics['test_loss'], label = 'test_loss')\n",
        "    #plt.legend()\n",
        "    #plt.show()\n",
        "\n",
        "    #plt.figure()\n",
        "    plt.plot(metrics['train_acc'], label = 'train_acc')\n",
        "    plt.plot(metrics['test_acc'], label = 'test_acc')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    \n",
        "    H = sess.run(hessian, feed_dict = {x: x_train, y: y_train})\n",
        "    eigens = np.real(np.linalg.eig(H)[0])\n",
        "    return metrics['train_acc'][-1], metrics['test_acc'][-1], eigens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nd-O5vq65kYS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def experiment_for_optimizer(gd, epochs = 1000, accuracy_threshold = 0.9, repetitions = 20):\n",
        "    \"\"\" Train repetitions copies of a network with optimizer, output\n",
        "    Hessian eigenvalues of those satisfying accuracy threshold\"\"\"\n",
        "\n",
        "    result = []\n",
        "    r = 0\n",
        "    while r < repetitions:\n",
        "        train_acc, test_acc, eigens = train_and_get_hessian(gd, epochs)\n",
        "        if train_acc < accuracy_threshold or test_acc < accuracy_threshold:\n",
        "            print('Error: accuracy is insufficient opt=%s train=%.2f test=%.2f' % (str(gd), train_acc, test_acc))\n",
        "            print('Done: %d/%d' % (r, repetitions))\n",
        "            continue\n",
        "        r += 1\n",
        "        print('Done: %d/%d' % (r, repetitions))\n",
        "        result += list(eigens)\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "deletable": false,
        "editable": false,
        "run_control": {
          "frozen": true
        },
        "id": "ZPYAYybQ5kYU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_acc, test_acc, eigens = train_and_get_hessian(gd, 1000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4zHogqcC5kYW",
        "colab_type": "text"
      },
      "source": [
        "## Experiments for Adam and GD, measuring spectrum of Hessian"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WEOFVAS5kYX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "adam = tf.train.AdamOptimizer().minimize(loss)\n",
        "eig_adam = experiment_for_optimizer(adam, epochs = 1000, accuracy_threshold = 0.9, repetitions = 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1-K2eOXs5kYY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gd = OwnGradientDescent(gamma = 0.5, theta = 0.999).minimize(loss, params)\n",
        "eig_gd = experiment_for_optimizer(gd, epochs = 1000, accuracy_threshold = 0.9, repetitions = 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zx5A-W6SGeMI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fw = FrankWolfe(p = 3.5).minimize(loss, params)\n",
        "eig_fw = experiment_for_optimizer(fw, epochs = 1000, accuracy_threshold = 0.9, repetitions = 20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6b02sY0u5kYb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def hist_th(eigens, threshold = 100, name = 'GD'):\n",
        "    plt.hist(np.extract(np.abs(eigens) < threshold, np.abs(eigens)), label = name, alpha = 0.5)\n",
        "    #plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdtq8VJ55kYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_for_thr(threshold = 100):\n",
        "    plt.figure()\n",
        "    hist_th(eig_gd, threshold, 'GD')\n",
        "    hist_th(eig_adam, threshold, 'Adam')\n",
        "    plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDRQzlQt5kYe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_for_thr(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvVco81n5kYh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_for_thr(100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "axC1KTZN5kYk",
        "colab_type": "text"
      },
      "source": [
        "Spectra are similar close to $0$, GD has a bit larger maximal eigenvalues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8q1athm5kYk",
        "colab_type": "text"
      },
      "source": [
        "### Gradient descent with different learning rates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rm3BfJWO5kYl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rates = np.linspace(0.2, 0.6, 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efKLTkvj5kYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def experiment_for_rate(rate, epochs = 1000, theta = 0.999):\n",
        "    \"\"\" Train repetitions copies of a network with GD(rate), output\n",
        "    Hessian eigenvalues of those satisfying accuracy threshold\"\"\"\n",
        "    \n",
        "    gd = OwnGradientDescent(gamma = rate, theta = theta).minimize(loss, params)\n",
        "    return experiment_for_optimizer(gd, epochs = epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZqSoo215kYo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "eigens_for_rate = {rate: experiment_for_rate(rate, epochs = 3000) for rate in learning_rates}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_HAydZu5kYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for r in learning_rates:\n",
        "    print(np.median(eigens_for_rate[r]), r)\n",
        "    hist_th(eigens_for_rate[r], threshold = 1, name = 'GD')\n",
        "    plt.legend()\n",
        "    #plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqSMnt8k5kYr",
        "colab_type": "text"
      },
      "source": [
        "Bigger learning rate results in smaller eigenvalues (more sharp minima)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VcKBGhx5kYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}